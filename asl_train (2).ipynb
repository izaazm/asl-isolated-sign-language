{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8mg3LiA8fJb",
        "outputId": "c1f33ed6-49d3-4440-d529-04f0eb18c9c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/591.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m583.7/591.0 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tf-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for leven (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow-addons==0.20.0\n",
        "!pip install -q git+https://github.com/hoyso48/tf-utils@main\n",
        "!pip install -q pyarrow leven"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtQMrGgh8fJd",
        "outputId": "6b8d135a-c8ea-49f7-ee87-c7d456a0fbaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow Version 2.12.0\n",
            "Python Version: 3.10.12 (main, Jun  7 2023, 12:45:35) [GCC 9.4.0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import pyarrow.parquet as pq\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as k\n",
        "import tensorflow_addons as tfa\n",
        "from tf_utils.schedules import OneCycleLR\n",
        "\n",
        "import glob\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "import gc\n",
        "import sys\n",
        "import sklearn\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "print(f'Tensorflow Version {tf.__version__}')\n",
        "print(f'Python Version: {sys.version}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4epuV5q8fJe"
      },
      "source": [
        "## Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjwohQGX8fJe",
        "outputId": "f5ce6a4f-c648-4454-8026-70c7fd00a210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n"
          ]
        }
      ],
      "source": [
        "CONFIG = {\"exp_name\": 'test',\n",
        "          \"device\": \"GPU\", # CPU, GPU, TPU-VM\n",
        "          \"max_len\" : 128,\n",
        "          \"epoch\": 50,\n",
        "          \"valid_size\": 0.1,\n",
        "          \"supplemental_size\": 0.5,\n",
        "          \"batch_size\": 128,\n",
        "          \"optimizer\": 'ranger', # 'ranger', 'adam'\n",
        "          \"loss\": 'ctc', # 'ctc', 'cce'\n",
        "          \"lr\": 1e-3,\n",
        "          \"lr_min\": 5e-7,\n",
        "          \"weight_decay\": 0.1,\n",
        "          \"warmup\": 0}\n",
        "\n",
        "!mkdir output\n",
        "root = '/content'\n",
        "root_save = '/content/output'\n",
        "print(CONFIG[\"exp_name\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CECd19478fJf",
        "outputId": "ecfb776c-dc11-4135-a872-50bdebc24fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU\n",
            "Num GPUs Available:  0\n",
            "REPLICAS: 1\n"
          ]
        }
      ],
      "source": [
        "def seed_everything(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "def get_strategy(device='TPU-VM'):\n",
        "    IS_TPU = False\n",
        "    if \"TPU\" in device:\n",
        "        tpu = 'local' if device=='TPU-VM' else None\n",
        "        print(\"connecting to TPU...\")\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        strategy = tf.distribute.TPUStrategy(tpu)\n",
        "        IS_TPU = True\n",
        "\n",
        "    if device == \"GPU\"  or device == \"CPU\":\n",
        "        ngpu = len(tf.config.experimental.list_physical_devices('GPU'))\n",
        "        if ngpu>1:\n",
        "            print(\"Using GPU\")\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "        elif ngpu==1:\n",
        "            print(\"Using single GPU\")\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "        else:\n",
        "            print(\"Using CPU\")\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "\n",
        "    if device == \"GPU\":\n",
        "        print(\"Num GPUs Available: \", ngpu)\n",
        "\n",
        "    AUTO     = tf.data.experimental.AUTOTUNE\n",
        "    REPLICAS = strategy.num_replicas_in_sync\n",
        "    print(f'REPLICAS: {REPLICAS}')\n",
        "\n",
        "    return strategy, REPLICAS, IS_TPU\n",
        "\n",
        "STRATEGY, N_REPLICAS, IS_TPU = get_strategy(CONFIG[\"device\"])\n",
        "CONFIG[\"batch_size\"] = CONFIG[\"batch_size\"] * N_REPLICAS\n",
        "CONFIG[\"lr\"] = CONFIG[\"lr\"] * N_REPLICAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy7qMnn98fJf"
      },
      "source": [
        "## Process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kInYKz28ihB",
        "outputId": "9783438d-f227-481e-c4e8-5aac7a7bec79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Files:  68\n",
            "Supplemental Train Files:  53\n",
            "CommandException: No URLs matched: gs://kds-7a1de6f6fb09cdad50ebab8364ce5c9e2937606d5437de561aab1448/train.csv\n",
            "CommandException: No URLs matched: gs://kds-7a1de6f6fb09cdad50ebab8364ce5c9e2937606d5437de561aab1448/supplemental_metadata.csv\n",
            "CommandException: No URLs matched: gs://kds-7a1de6f6fb09cdad50ebab8364ce5c9e2937606d5437de561aab1448/character_to_prediction_index.json\n"
          ]
        }
      ],
      "source": [
        "GCS_PATH = {'asl-fingerspelling':'gs://kds-838fb40869e092ab9936d6e90a7b7c43c26bb07471fcd3be7d29707a',\n",
        "            'asl-tfr':'gs://kds-26a3f077e7100aecb626e6fecf52e9044e6382c5b31f0caf36be9290',\n",
        "            'asl-tfr-supplemental':'gs://kds-ef88fb86efbd26aba9ea51c72e42e39f8868f1b95529eae69e51701b'}\n",
        "\n",
        "TRAIN_FILENAMES = tf.io.gfile.glob(GCS_PATH['asl-tfr']+'/train/*.tfrecord')\n",
        "TRAIN_FILENAMES_SUPPLEMENTAL = tf.io.gfile.glob(GCS_PATH['asl-tfr-supplemental']+'/supplemental/*.tfrecord')\n",
        "COMPETITION_PATH = GCS_PATH['asl-fingerspelling']\n",
        "\n",
        "print(\"Train Files: \", len(TRAIN_FILENAMES))\n",
        "print(\"Supplemental Train Files: \", len(TRAIN_FILENAMES_SUPPLEMENTAL))\n",
        "!gsutil cp {COMPETITION_PATH}/train.csv .\n",
        "!gsutil cp {COMPETITION_PATH}/supplemental_metadata.csv .\n",
        "!gsutil cp {COMPETITION_PATH}/character_to_prediction_index.json ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKo2oDqW8fJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5bb978-18f0-4631-a5c9-9d298e7ae617"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-260133e47609>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{root}/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_supplemental\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{root}/supplemental_metadata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mNUM_DATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"
          ]
        }
      ],
      "source": [
        "train = pd.read_csv(f'{root}/train.csv')\n",
        "train_supplemental = pd.read_csv(f'{root}/supplemental_metadata.csv')\n",
        "NUM_DATA = len(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4nwRAMN8fJf"
      },
      "outputs": [],
      "source": [
        "LIP = [\n",
        "    61, 185, 40, 39, 37, 267, 269, 270, 409, 291,\n",
        "    146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
        "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
        "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
        "]\n",
        "\n",
        "NOSE = [1, 2, 98, 327]\n",
        "\n",
        "LEYE = [466, 388, 387, 386, 385, 384, 398,\n",
        "        263, 249, 390, 373, 374, 380, 381, 382, 362]\n",
        "\n",
        "REYE = [246, 161, 160, 159, 158, 157, 173,\n",
        "        33, 7, 163, 144, 145, 153, 154, 155, 133]\n",
        "\n",
        "LPOSE = [13, 15, 17, 19, 21]\n",
        "RPOSE = [14, 16, 18, 20, 22]\n",
        "POSE = LPOSE + RPOSE\n",
        "\n",
        "LIP_ = [f'x_face_{i}' for i in LIP] + [f'y_face_{i}' for i in LIP] + [f'z_face_{i}' for i in LIP]\n",
        "NOSE_ = [f'x_face_{i}' for i in NOSE] + [f'y_face_{i}' for i in NOSE] + [f'z_face_{i}' for i in NOSE]\n",
        "LEYE_ = [f'x_face_{i}' for i in LEYE] + [f'y_face_{i}' for i in LEYE] + [f'z_face_{i}' for i in LEYE]\n",
        "REYE_ = [f'x_face_{i}' for i in REYE] + [f'y_face_{i}' for i in REYE] + [f'z_face_{i}' for i in REYE]\n",
        "\n",
        "LHAND_ = [f'x_left_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)]\n",
        "RHAND_ = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\n",
        "POSE_ = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n",
        "\n",
        "FEATURE_COL = LIP_ + NOSE_ + LEYE_ + REYE_ + LHAND_ + RHAND_ + POSE_\n",
        "FRAME_LEN = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vdtXCyc8fJg"
      },
      "outputs": [],
      "source": [
        "X_IDX = [i for i, col in enumerate(FEATURE_COL) if \"x_\" in col]\n",
        "Y_IDX = [i for i, col in enumerate(FEATURE_COL) if \"y_\" in col]\n",
        "Z_IDX = [i for i, col in enumerate(FEATURE_COL) if \"z_\" in col]\n",
        "\n",
        "RHAND_IDX = [i for i, col in enumerate(FEATURE_COL) if \"right_hand\" in col]\n",
        "LHAND_IDX = [i for i, col in enumerate(FEATURE_COL) if \"left_hand\" in col]\n",
        "RPOSE_IDX = [i for i, col in enumerate(FEATURE_COL) if \"pose\" in col and int(col.split(\"_\")[-1]) in RPOSE]\n",
        "LPOSE_IDX = [i for i, col in enumerate(FEATURE_COL) if \"pose\" in col and int(col.split(\"_\")[-1]) in LPOSE]\n",
        "LIP_IDX = [i for i, col in enumerate(FEATURE_COL) if \"face\" in col and int(col.split(\"_\")[-1]) in LIP]\n",
        "NOSE_IDX = [i for i, col in enumerate(FEATURE_COL) if \"face\" in col and int(col.split(\"_\")[-1]) in NOSE]\n",
        "LEYE_IDX = [i for i, col in enumerate(FEATURE_COL) if \"face\" in col and int(col.split(\"_\")[-1]) in LEYE]\n",
        "REYE_IDX = [i for i, col in enumerate(FEATURE_COL) if \"face\" in col and int(col.split(\"_\")[-1]) in REYE]\n",
        "# NORM_IDX = [i for i, col in enumerate(FEATURE_COL) if \"pose\" == col and int(col[-2:]) in LPOSE]\n",
        "\n",
        "print(NOSE_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwJ5DLAj8fJg"
      },
      "outputs": [],
      "source": [
        "with open(f'{root}/character_to_prediction_index.json') as json_file:\n",
        "    CHAR2ORD = json.load(json_file)\n",
        "\n",
        "# Character to Ordinal Encoding Mapping\n",
        "# display(pd.Series(CHAR2ORD).to_frame('Ordinal Encoding'))\n",
        "\n",
        "pad_token = '^'\n",
        "PAD_TOKEN_IDX = 59\n",
        "CHAR2ORD[pad_token] = PAD_TOKEN_IDX\n",
        "ORD2CHAR = {j:i for i,j in CHAR2ORD.items()}\n",
        "print(len(CHAR2ORD))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3xXXcFW_2MJ"
      },
      "source": [
        "## Preprocessing\n",
        "lots of todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeohRkbr8fJg"
      },
      "outputs": [],
      "source": [
        "## TODO\n",
        "\n",
        "class Preprocess(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_len=128, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZzUrgdo8fJg"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# still the same as starter\n",
        "def resize_pad(x, max_len):\n",
        "    if tf.shape(x)[0] < max_len:\n",
        "        x = tf.pad(x, ([[0, max_len-tf.shape(x)[0]], [0, 0], [0, 0]]))\n",
        "    else:\n",
        "        x = tf.image.resize(x, (max_len, tf.shape(x)[1]))\n",
        "    return x\n",
        "\n",
        "## TODO\n",
        "def augment_fn(x, max_len):\n",
        "    return x\n",
        "\n",
        "def stack_to_3d(x, IDX, norm=False):\n",
        "    x_x = x[:, 0*(len(IDX)//3) : 1*(len(IDX)//3)]\n",
        "    x_y = x[:, 1*(len(IDX)//3) : 2*(len(IDX)//3)]\n",
        "    x_z = x[:, 2*(len(IDX)//3) : 3*(len(IDX)//3)]\n",
        "    x = tf.concat([x_x[..., tf.newaxis], x_y[..., tf.newaxis], x_z[..., tf.newaxis]], axis=-1)\n",
        "    if norm:\n",
        "        mean = tf.math.reduce_mean(x, axis=1)[:, tf.newaxis, :]\n",
        "        std = tf.math.reduce_std(x, axis=1)[:, tf.newaxis, :]\n",
        "        x = (x - mean) / std\n",
        "    return x\n",
        "\n",
        "# Detect the dominant hand from the number of NaN values.\n",
        "# Dominant hand will have less NaN values since it is in frame moving.\n",
        "def preprocess(landmarks, phrase, augment=True, max_len=64, norm=False):\n",
        "    x = landmarks\n",
        "    # lip = tf.gather(x, tf.cast(LIP_IDX, tf.int32), axis=1)\n",
        "    # nose = tf.gather(x, tf.cast(NOSE_IDX, tf.int32), axis=1)\n",
        "    # leye = tf.gather(x, tf.cast(LEYE_IDX, tf.int32), axis=1)\n",
        "    # reye = tf.gather(x, tf.cast(REYE_IDX, tf.int32), axis=1)\n",
        "    rhand = tf.gather(x, tf.cast(RHAND_IDX, tf.int32), axis=1)\n",
        "    lhand = tf.gather(x, tf.cast(LHAND_IDX, tf.int32), axis=1)\n",
        "    rpose = tf.gather(x, tf.cast(RPOSE_IDX, tf.int32), axis=1)\n",
        "    lpose = tf.gather(x, tf.cast(LPOSE_IDX, tf.int32), axis=1)\n",
        "\n",
        "    # lip = stack_to_3d(lip, LIP_IDX, norm=norm)\n",
        "    # nose = stack_to_3d(nose, NOSE_IDX, norm=norm)\n",
        "    # leye = stack_to_3d(leye, LEYE_IDX, norm=norm)\n",
        "    # reye = stack_to_3d(reye, REYE_IDX, norm=norm)\n",
        "    rhand = stack_to_3d(rhand, RHAND_IDX, norm=norm)\n",
        "    lhand = stack_to_3d(lhand, LHAND_IDX, norm=norm)\n",
        "    rpose = stack_to_3d(rpose, RPOSE_IDX, norm=norm)\n",
        "    lpose = stack_to_3d(lpose, LPOSE_IDX, norm=norm)\n",
        "\n",
        "    x = tf.concat([rhand, lhand, rpose, lpose], axis=1)\n",
        "    if augment:\n",
        "        x = augment_fn(x, max_len=max_len)\n",
        "\n",
        "    print(x.shape)\n",
        "    x = resize_pad(x, max_len)\n",
        "\n",
        "    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
        "    x = tf.reshape(x, (FRAME_LEN, len(LHAND_ + RHAND_ + POSE_)))\n",
        "    return x, phrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8d6odxn8fJg"
      },
      "outputs": [],
      "source": [
        "table = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=list(CHAR2ORD.keys()),\n",
        "        values=list(CHAR2ORD.values()),\n",
        "    ),\n",
        "    default_value=tf.constant(-1),\n",
        "    name=\"class_weight\"\n",
        ")\n",
        "\n",
        "def decode_fn(record_bytes):\n",
        "    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in FEATURE_COL}\n",
        "    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n",
        "    features = tf.io.parse_single_example(record_bytes, schema)\n",
        "    phrase = features[\"phrase\"]\n",
        "    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in FEATURE_COL])\n",
        "    # Transpose to maintain the original shape of landmarks data.\n",
        "    landmarks = tf.transpose(landmarks)\n",
        "\n",
        "    return landmarks, phrase\n",
        "\n",
        "def convert_fn(landmarks, phrase):\n",
        "    # Add start and end pointers to phrase.\n",
        "    phrase = tf.strings.bytes_split(phrase)\n",
        "    phrase = table.lookup(phrase)\n",
        "    # Vectorize and add padding.\n",
        "    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode = 'CONSTANT',\n",
        "                    constant_values = PAD_TOKEN_IDX)\n",
        "#     t1 = [SOS_TOKEN]\n",
        "#     t2 = [EOS_TOKEN]\n",
        "#     phrase = tf.concat([t1, phrase, t2], axis=0)\n",
        "\n",
        "    return landmarks, phrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVhjCVO68fJg"
      },
      "outputs": [],
      "source": [
        "# see tfr data\n",
        "for file_id in train.file_id:\n",
        "    tffile = f\"{GCS_PATH['asl-tfr']}/train/{file_id}.tfrecord\"\n",
        "    for batch in tf.data.TFRecordDataset([tffile], num_parallel_reads=tf.data.AUTOTUNE, compression_type='GZIP').map(decode_fn).take(2):\n",
        "        print(list(batch)[1])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HX3_7OR8fJg"
      },
      "source": [
        "## Get Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYIf2tN98fJh"
      },
      "outputs": [],
      "source": [
        "PAD = -100\n",
        "def get_tfrec_dataset(tfrecords, batch_size=64, max_len=64, cache=True, drop_remainder=False, augment=False, shuffle=False, repeat=False):\n",
        "    print(\"# Getting Dataset\")\n",
        "    ds = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=tf.data.AUTOTUNE, compression_type='GZIP')\n",
        "    ds = ds.map(decode_fn, tf.data.AUTOTUNE)\n",
        "    ds = ds.map(convert_fn, tf.data.AUTOTUNE)\n",
        "    ds = ds.map(lambda landmarks, phrase: preprocess(landmarks, phrase, augment=augment, max_len=max_len), tf.data.AUTOTUNE)\n",
        "\n",
        "    if repeat:\n",
        "        ds = ds.repeat()\n",
        "\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(shuffle)\n",
        "        options = tf.data.Options()\n",
        "        options.experimental_deterministic = (False)\n",
        "        ds = ds.with_options(options)\n",
        "\n",
        "    if IS_TPU:\n",
        "        ds = ds.padded_batch(batch_size,\n",
        "                             padding_values=(tf.constant(PAD, dtype=tf.float32),\n",
        "                                             tf.constant(PAD_TOKEN_IDX, dtype=tf.int32)),\n",
        "                             padded_shapes=([FRAME_LEN, len(FEATURE_COL)], [64]),\n",
        "                             drop_remainder=drop_remainder)\n",
        "    else:\n",
        "        ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
        "\n",
        "    df = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    if cache:\n",
        "        ds = ds.cache()\n",
        "\n",
        "    print(\"# Done\")\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL8OQlpe8fJh"
      },
      "outputs": [],
      "source": [
        "print(f\"Train: {len(TRAIN_FILENAMES)} TFRecord files.\")\n",
        "print(f\"Train Supplemental: {len(TRAIN_FILENAMES_SUPPLEMENTAL)} TFRecord files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CbE8V0__-Tr"
      },
      "source": [
        "see the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juBr1caO8fJh"
      },
      "outputs": [],
      "source": [
        "dummy_d = get_tfrec_dataset(TRAIN_FILENAMES[-1:],\n",
        "                            batch_size=CONFIG[\"batch_size\"],\n",
        "                            max_len=CONFIG[\"max_len\"])\n",
        "\n",
        "batch_test = next(iter(dummy_d))\n",
        "print(list(batch_test)[0][0].shape, list(batch_test)[1][0])\n",
        "INPUT_SHAPE = list(list(batch_test)[0][0].shape)\n",
        "print(INPUT_SHAPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iZcQhPs8fJh"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj1gRklE8fJh"
      },
      "outputs": [],
      "source": [
        "def CTCLoss(labels, logits):\n",
        "    label_length = tf.reduce_sum(tf.cast(labels != PAD_TOKEN_IDX, tf.int32), axis=-1)\n",
        "    if not IS_TPU:\n",
        "        logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n",
        "    else:\n",
        "        logit_length = tf.ones(CONFIG['batch_size'], dtype=tf.int32) * tf.cast(64, tf.int32)\n",
        "    loss = tf.nn.ctc_loss(\n",
        "            labels=labels,\n",
        "            logits=logits,\n",
        "            label_length=label_length,\n",
        "            logit_length=logit_length,\n",
        "            blank_index=PAD_TOKEN_IDX,\n",
        "            logits_time_major=False\n",
        "        )\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Hb2qiUr8fJh"
      },
      "outputs": [],
      "source": [
        "def num_to_char_fn(y):\n",
        "    return [ORD2CHAR.get(x, \"\") for x in y]\n",
        "\n",
        "@tf.function()\n",
        "def decode_phrase(pred):\n",
        "    x = tf.argmax(pred, axis=1)\n",
        "    diff = tf.not_equal(x[:-1], x[1:])\n",
        "    adjacent_indices = tf.where(diff)[:, 0]\n",
        "    x = tf.gather(x, adjacent_indices)\n",
        "    mask = x != PAD_TOKEN_IDX\n",
        "    x = tf.boolean_mask(x, mask, axis=0)\n",
        "    return x\n",
        "\n",
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    output_text = []\n",
        "    for result in pred:\n",
        "        result = \"\".join(num_to_char_fn(decode_phrase(result).numpy()))\n",
        "        output_text.append(result)\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgCjtoXy8fJh"
      },
      "outputs": [],
      "source": [
        "class CallbackEval(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset, freq=5):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.freq = freq\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs=None):\n",
        "        if (epoch + 1) % self.freq == 0:\n",
        "            predictions = []\n",
        "            targets = []\n",
        "            for batch in self.dataset:\n",
        "                X, y = batch\n",
        "                batch_predictions = self.model(X)\n",
        "                batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "                predictions.extend(batch_predictions)\n",
        "                for label in y:\n",
        "                    label = \"\".join(num_to_char_fn(label.numpy()))\n",
        "                    targets.append(label)\n",
        "            print(\"-\" * 100)\n",
        "            # for i in np.random.randint(0, len(predictions), 2):\n",
        "            for i in range(32):\n",
        "                print(f\"Target    : {targets[i]}\")\n",
        "                print(f\"Prediction: {predictions[i]}, len: {len(predictions[i])}\")\n",
        "                print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03CimuvF8fJh"
      },
      "outputs": [],
      "source": [
        "class ClearMemory(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print()\n",
        "        gc.collect()\n",
        "        k.clear_session()\n",
        "        print(\"Cleared epoch session\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdXfvgZG8fJh"
      },
      "source": [
        "## Get Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F59nCsTK8fJh"
      },
      "outputs": [],
      "source": [
        "#Copied from previous comp 1st place model: https://www.kaggle.com/code/hoyso48/1st-place-solution-training\n",
        "class ECA(tf.keras.layers.Layer):\n",
        "    def __init__(self, kernel_size=5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
        "        nn = tf.expand_dims(nn, -1)\n",
        "        nn = self.conv(nn)\n",
        "        nn = tf.squeeze(nn, -1)\n",
        "        nn = tf.nn.sigmoid(nn)\n",
        "        nn = nn[:,None,:]\n",
        "        return inputs * nn\n",
        "\n",
        "class CausalDWConv1D(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "        kernel_size=17,\n",
        "        dilation_rate=1,\n",
        "        use_bias=False,\n",
        "        depthwise_initializer='glorot_uniform',\n",
        "        name='', **kwargs):\n",
        "        super().__init__(name=name,**kwargs)\n",
        "        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
        "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
        "                            kernel_size,\n",
        "                            strides=1,\n",
        "                            dilation_rate=dilation_rate,\n",
        "                            padding='valid',\n",
        "                            use_bias=use_bias,\n",
        "                            depthwise_initializer=depthwise_initializer,\n",
        "                            name=name + '_dwconv')\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.causal_pad(inputs)\n",
        "        x = self.dw_conv(x)\n",
        "        return x\n",
        "\n",
        "def Conv1DBlock(channel_size,\n",
        "          kernel_size,\n",
        "          dilation_rate=1,\n",
        "          drop_rate=0.0,\n",
        "          expand_ratio=2,\n",
        "          se_ratio=0.25,\n",
        "          activation='swish',\n",
        "          name=None):\n",
        "    '''\n",
        "    efficient conv1d block, @hoyso48\n",
        "    '''\n",
        "    if name is None:\n",
        "        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
        "    # Expansion phase\n",
        "    def apply(inputs):\n",
        "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
        "        channels_expand = channels_in * expand_ratio\n",
        "\n",
        "        skip = inputs\n",
        "\n",
        "        x = tf.keras.layers.Dense(\n",
        "            channels_expand,\n",
        "            use_bias=True,\n",
        "            activation=activation,\n",
        "            name=name + '_expand_conv')(inputs)\n",
        "\n",
        "        # Depthwise Convolution\n",
        "        x = CausalDWConv1D(kernel_size,\n",
        "            dilation_rate=dilation_rate,\n",
        "            use_bias=False,\n",
        "            name=name + '_dwconv')(x)\n",
        "\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn')(x)\n",
        "\n",
        "        x  = ECA()(x)\n",
        "\n",
        "        x = tf.keras.layers.Dense(\n",
        "            channel_size,\n",
        "            use_bias=True,\n",
        "            name=name + '_project_conv')(x)\n",
        "\n",
        "        if drop_rate > 0:\n",
        "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n",
        "\n",
        "        if (channels_in == channel_size):\n",
        "            x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.scale = self.dim ** -0.5\n",
        "        self.num_heads = num_heads\n",
        "        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
        "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        qkv = self.qkv(inputs)\n",
        "        qkv = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n",
        "        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
        "\n",
        "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, None, :]\n",
        "\n",
        "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
        "        attn = self.drop1(attn)\n",
        "\n",
        "        x = attn @ v\n",
        "        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def TransformerBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish'):\n",
        "    def apply(inputs):\n",
        "        x = inputs\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
        "        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout)(x)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
        "        x = tf.keras.layers.Add()([inputs, x])\n",
        "        attn_out = x\n",
        "\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
        "        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation)(x)\n",
        "        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
        "        x = tf.keras.layers.Add()([attn_out, x])\n",
        "        return x\n",
        "    return apply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPLZH9BY8fJh"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q98b-Gwe8fJh"
      },
      "outputs": [],
      "source": [
        "def get_model(dim = 384, dropout_step=0):\n",
        "    inp = tf.keras.Input(INPUT_SHAPE)\n",
        "    x = inp\n",
        "    ksize = 11\n",
        "    x = tf.keras.layers.Masking(mask_value=PAD,input_shape=INPUT_SHAPE)(inp)\n",
        "    x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(momentum=0.95,name='stem_bn')(x)\n",
        "\n",
        "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
        "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
        "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
        "    x = TransformerBlock(dim,expand=2)(x)\n",
        "\n",
        "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
        "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
        "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
        "    x = TransformerBlock(dim,expand=2)(x)\n",
        "    x = TransformerBlock(dim,expand=2)(x)\n",
        "\n",
        "#     x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
        "#     x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
        "#     x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
        "#     x = TransformerBlock(dim,expand=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(dim*2,activation=None,name='top_conv')(x)\n",
        "    x = tf.keras.layers.AveragePooling1D(2)(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Dense(len(CHAR2ORD))(x)\n",
        "\n",
        "    model = tf.keras.Model(inp, x)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B24UENI08fJh",
        "outputId": "ee40ee74-7a44-401a-e326-74f051d0eafe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Shape (input, target) (128, 128, 156) (128, 64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/ctc_ops.py:1512: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/ctc_ops.py:1495: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Shape (128, 64, 60)\n",
            "CTC Loss tf.Tensor(195.8284, shape=(), dtype=float32)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128, 156)]   0           []                               \n",
            "                                                                                                  \n",
            " masking (Masking)              (None, 128, 156)     0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " stem_conv (Dense)              (None, 128, 384)     59904       ['masking[0][0]']                \n",
            "                                                                                                  \n",
            " stem_bn (BatchNormalization)   (None, 128, 384)     1536        ['stem_conv[0][0]']              \n",
            "                                                                                                  \n",
            " 1_expand_conv (Dense)          (None, 128, 768)     295680      ['stem_bn[0][0]']                \n",
            "                                                                                                  \n",
            " 1_dwconv (CausalDWConv1D)      (None, 128, 768)     8448        ['1_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 1_bn (BatchNormalization)      (None, 128, 768)     3072        ['1_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca (ECA)                      (None, 128, 768)     5           ['1_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 1_project_conv (Dense)         (None, 128, 384)     295296      ['eca[0][0]']                    \n",
            "                                                                                                  \n",
            " 1_drop (Dropout)               (None, 128, 384)     0           ['1_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 1_add (Add)                    (None, 128, 384)     0           ['1_drop[0][0]',                 \n",
            "                                                                  'stem_bn[0][0]']                \n",
            "                                                                                                  \n",
            " 2_expand_conv (Dense)          (None, 128, 768)     295680      ['1_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 2_dwconv (CausalDWConv1D)      (None, 128, 768)     8448        ['2_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 2_bn (BatchNormalization)      (None, 128, 768)     3072        ['2_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca_1 (ECA)                    (None, 128, 768)     5           ['2_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 2_project_conv (Dense)         (None, 128, 384)     295296      ['eca_1[0][0]']                  \n",
            "                                                                                                  \n",
            " 2_drop (Dropout)               (None, 128, 384)     0           ['2_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 2_add (Add)                    (None, 128, 384)     0           ['2_drop[0][0]',                 \n",
            "                                                                  '1_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 3_expand_conv (Dense)          (None, 128, 768)     295680      ['2_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 3_dwconv (CausalDWConv1D)      (None, 128, 768)     8448        ['3_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 3_bn (BatchNormalization)      (None, 128, 768)     3072        ['3_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca_2 (ECA)                    (None, 128, 768)     5           ['3_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 3_project_conv (Dense)         (None, 128, 384)     295296      ['eca_2[0][0]']                  \n",
            "                                                                                                  \n",
            " 3_drop (Dropout)               (None, 128, 384)     0           ['3_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 3_add (Add)                    (None, 128, 384)     0           ['3_drop[0][0]',                 \n",
            "                                                                  '2_add[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 128, 384)    1536        ['3_add[0][0]']                  \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_self_attention (Mul  (None, 128, 384)    589824      ['batch_normalization[0][0]']    \n",
            " tiHeadSelfAttention)                                                                             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128, 384)     0           ['multi_head_self_attention[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 128, 384)     0           ['3_add[0][0]',                  \n",
            "                                                                  'dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 128, 384)    1536        ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 128, 768)     294912      ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 128, 384)     294912      ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 128, 384)     0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 128, 384)     0           ['add[0][0]',                    \n",
            "                                                                  'dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " 4_expand_conv (Dense)          (None, 128, 768)     295680      ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " 4_dwconv (CausalDWConv1D)      (None, 128, 768)     8448        ['4_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 4_bn (BatchNormalization)      (None, 128, 768)     3072        ['4_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca_3 (ECA)                    (None, 128, 768)     5           ['4_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 4_project_conv (Dense)         (None, 128, 384)     295296      ['eca_3[0][0]']                  \n",
            "                                                                                                  \n",
            " 4_drop (Dropout)               (None, 128, 384)     0           ['4_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 4_add (Add)                    (None, 128, 384)     0           ['4_drop[0][0]',                 \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " 5_expand_conv (Dense)          (None, 128, 768)     295680      ['4_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 5_dwconv (CausalDWConv1D)      (None, 128, 768)     8448        ['5_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 5_bn (BatchNormalization)      (None, 128, 768)     3072        ['5_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca_4 (ECA)                    (None, 128, 768)     5           ['5_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 5_project_conv (Dense)         (None, 128, 384)     295296      ['eca_4[0][0]']                  \n",
            "                                                                                                  \n",
            " 5_drop (Dropout)               (None, 128, 384)     0           ['5_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 5_add (Add)                    (None, 128, 384)     0           ['5_drop[0][0]',                 \n",
            "                                                                  '4_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 6_expand_conv (Dense)          (None, 128, 768)     295680      ['5_add[0][0]']                  \n",
            "                                                                                                  \n",
            " 6_dwconv (CausalDWConv1D)      (None, 128, 768)     8448        ['6_expand_conv[0][0]']          \n",
            "                                                                                                  \n",
            " 6_bn (BatchNormalization)      (None, 128, 768)     3072        ['6_dwconv[0][0]']               \n",
            "                                                                                                  \n",
            " eca_5 (ECA)                    (None, 128, 768)     5           ['6_bn[0][0]']                   \n",
            "                                                                                                  \n",
            " 6_project_conv (Dense)         (None, 128, 384)     295296      ['eca_5[0][0]']                  \n",
            "                                                                                                  \n",
            " 6_drop (Dropout)               (None, 128, 384)     0           ['6_project_conv[0][0]']         \n",
            "                                                                                                  \n",
            " 6_add (Add)                    (None, 128, 384)     0           ['6_drop[0][0]',                 \n",
            "                                                                  '5_add[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 128, 384)    1536        ['6_add[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_self_attention_1 (M  (None, 128, 384)    589824      ['batch_normalization_2[0][0]']  \n",
            " ultiHeadSelfAttention)                                                                           \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 128, 384)     0           ['multi_head_self_attention_1[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 128, 384)     0           ['6_add[0][0]',                  \n",
            "                                                                  'dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 128, 384)    1536        ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 128, 768)     294912      ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 128, 384)     294912      ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 128, 384)     0           ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 128, 384)     0           ['add_2[0][0]',                  \n",
            "                                                                  'dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 128, 384)    1536        ['add_3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_self_attention_2 (M  (None, 128, 384)    589824      ['batch_normalization_4[0][0]']  \n",
            " ultiHeadSelfAttention)                                                                           \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 128, 384)     0           ['multi_head_self_attention_2[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 128, 384)     0           ['add_3[0][0]',                  \n",
            "                                                                  'dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 128, 384)    1536        ['add_4[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 128, 768)     294912      ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 128, 384)     294912      ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 128, 384)     0           ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 128, 384)     0           ['add_4[0][0]',                  \n",
            "                                                                  'dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " top_conv (Dense)               (None, 128, 768)     295680      ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " average_pooling1d (AveragePool  (None, 64, 768)     0           ['top_conv[0][0]']               \n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 64, 768)      0           ['average_pooling1d[0][0]']      \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 64, 60)       46140       ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7,566,426\n",
            "Trainable params: 7,551,834\n",
            "Non-trainable params: 14,592\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model_test = get_model()\n",
        "print(\"Data Shape (input, target)\", batch_test[0].shape, batch_test[1].shape)\n",
        "y_hat = model_test(batch_test[0])\n",
        "print(\"Output Shape\", y_hat.shape)\n",
        "print(\"CTC Loss\", CTCLoss(batch_test[1], y_hat))\n",
        "model_test.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--yxklOd8fJi"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cmK-SEECTJx"
      },
      "outputs": [],
      "source": [
        "TRAIN_CONFIG = {\"print_transcription\": True,\n",
        "                \"float16\": False,\n",
        "                \"verbose\": 1,\n",
        "                \"seeds\": [42],\n",
        "                \"fold\": 'part',\n",
        "                \"clear_memory\": True}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S0TT0Tp8fJi"
      },
      "outputs": [],
      "source": [
        "def train_fold(seed, train_files, valid_files, strategy=None):\n",
        "    # set seed\n",
        "    print(\"### Training with seed:\", {seed}, \" data: \", TRAIN_CONFIG['fold'])\n",
        "    print(\"# Seeding\")\n",
        "    seed_everything(seed=seed)\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "    tf.config.optimizer.set_jit(True)\n",
        "\n",
        "    if TRAIN_CONFIG['float16']:\n",
        "        try:\n",
        "            policy = tf.keras.mixed_precision.Policy('mixed_bfloat16')\n",
        "            tf.keras.mixed_precision.set_global_policy(policy)\n",
        "        except:\n",
        "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "            tf.keras.mixed_precision.set_global_policy(policy)\n",
        "    else:\n",
        "        policy = tf.keras.mixed_precision.Policy('float32')\n",
        "        tf.keras.mixed_precision.set_global_policy(policy)\n",
        "\n",
        "    if valid_files is not None:\n",
        "        train_dataset = get_tfrec_dataset(train_files,\n",
        "                                          batch_size=CONFIG[\"batch_size\"],\n",
        "                                          max_len=CONFIG[\"max_len\"], cache=False,\n",
        "                                          drop_remainder=True, augment=True,\n",
        "                                          repeat=False, shuffle=False)\n",
        "        valid_dataset = get_tfrec_dataset(valid_files,\n",
        "                                          batch_size=CONFIG[\"batch_size\"],\n",
        "                                          max_len=CONFIG[\"max_len\"], cache=False,\n",
        "                                          drop_remainder=False, augment=False,\n",
        "                                          repeat=False, shuffle=False)\n",
        "    else:\n",
        "        train_dataset = get_tfrec_dataset(train_files,\n",
        "                                          batch_size=CONFIG[\"batch_size\"],\n",
        "                                          max_len=CONFIG[\"max_len\"], cache=False,\n",
        "                                          drop_remainder=False, augment=True,\n",
        "                                          repeat=False, shuffle=False)\n",
        "        valid_dataset = None\n",
        "\n",
        "    with strategy.scope():\n",
        "        print(\"# Getting model\")\n",
        "        model=get_model()\n",
        "\n",
        "        schedule = OneCycleLR(CONFIG[\"lr\"], CONFIG[\"epoch\"],\n",
        "                                  warmup_epochs=CONFIG[\"epoch\"]*CONFIG[\"warmup\"],\n",
        "                                  steps_per_epoch=314, decay_epochs=CONFIG[\"epoch\"],\n",
        "                                  lr_min=CONFIG[\"lr_min\"],\n",
        "                                  decay_type='cosine',\n",
        "                                  warmup_type='linear')\n",
        "        decay_schedule = OneCycleLR((CONFIG[\"lr\"])*CONFIG[\"weight_decay\"], CONFIG[\"epoch\"],\n",
        "                                    warmup_epochs=CONFIG[\"epoch\"]*CONFIG[\"warmup\"],\n",
        "                                    steps_per_epoch=314, decay_epochs=CONFIG[\"epoch\"],\n",
        "                                    lr_min=CONFIG[\"lr_min\"]*CONFIG[\"weight_decay\"],\n",
        "                                    decay_type='cosine',\n",
        "                                    warmup_type='linear')\n",
        "\n",
        "        if CONFIG['optimizer'] == \"ranger\":\n",
        "            optimizer = tfa.optimizers.RectifiedAdam(learning_rate=schedule, weight_decay=decay_schedule, sma_threshold=4)\n",
        "            optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=5)\n",
        "        elif CONFIG['optimizer'] == \"adam\":\n",
        "            optimizer = keras.optimizers.Adam(learning_rate=schedule, weight_decay=CONFIG['weight_decay'])\n",
        "\n",
        "        if CONFIG['loss'] == \"cce\":\n",
        "            loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\n",
        "        elif CONFIG['loss'] == \"ctc\":\n",
        "            loss = CTCLoss\n",
        "\n",
        "        print(\"# Compiling model\")\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=loss,\n",
        "                      run_eagerly=False)\n",
        "        print(\"# Done\")\n",
        "\n",
        "    print(\"# Starting training\")\n",
        "\n",
        "    ## callbacks\n",
        "    clear_memory = ClearMemory()\n",
        "    logger = tf.keras.callbacks.CSVLogger(f'{root_save}/fold{seed}-logs.csv')\n",
        "    sv_loss0 = tf.keras.callbacks.ModelCheckpoint(f'{root_save}/fold{seed}-best.h5', monitor='val_loss', verbose=0, save_best_only=True,\n",
        "                                                  save_weights_only=True, mode='min', save_freq='epoch')\n",
        "    sv_loss1 = tf.keras.callbacks.ModelCheckpoint(f'{root_save}/fold{seed}-best.h5', monitor='loss', verbose=0, save_best_only=True,\n",
        "                                                  save_weights_only=True, mode='min', save_freq='epoch')\n",
        "    validation_callback = CallbackEval(dummy_d.take(1), freq=3)\n",
        "\n",
        "    callbacks=[logger]\n",
        "    if valid_dataset is not None:\n",
        "        callbacks.append(sv_loss0)\n",
        "    else:\n",
        "        callbacks.append(sv_loss1)\n",
        "\n",
        "    if TRAIN_CONFIG[\"print_transcription\"]:\n",
        "        callbacks.append(validation_callback)\n",
        "    if TRAIN_CONFIG[\"clear_memory\"]:\n",
        "        callbacks.append(clear_memory)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=valid_dataset,\n",
        "        epochs=CONFIG[\"epoch\"],\n",
        "        # steps_per_epoch=steps_per_epoch,\n",
        "        callbacks=callbacks,\n",
        "        verbose=TRAIN_CONFIG['verbose'])\n",
        "\n",
        "    try:\n",
        "        model.load_weights(f'{root_save}/fold{seed}-best.h5')\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if valid_dataset is not None:\n",
        "        val = model.evaluate(valid_dataset, verbose=TRAIN_CONFIG['verbose'])\n",
        "    else:\n",
        "        val = None\n",
        "\n",
        "    del model, train_dataset, valid_dataset\n",
        "    _ = gc.collect()\n",
        "    return history, val\n",
        "\n",
        "def run_training():\n",
        "    history = {}\n",
        "    val = {}\n",
        "    for seed in TRAIN_CONFIG['seeds']:\n",
        "        if TRAIN_CONFIG['fold'] == 'all':\n",
        "            train_files = TRAIN_FILENAMES + TRAIN_FILENAMES_SUPPLEMENTAL[:int(len(TRAIN_FILENAMES_SUPPLEMENTAL)*CONFIG['supplemental_size'])]\n",
        "            history_fold, val_fold = train_fold(seed=seed, train_files=train_files, valid_files=None, strategy=STRATEGY)\n",
        "        elif TRAIN_CONFIG['fold'] == \"part\":\n",
        "            len_train = int(len(TRAIN_FILENAMES) * (1 - CONFIG[\"valid_size\"]))\n",
        "            train_files = TRAIN_FILENAMES[:len_train] + TRAIN_FILENAMES_SUPPLEMENTAL[:int(len(TRAIN_FILENAMES_SUPPLEMENTAL)*CONFIG['supplemental_size'])]\n",
        "            valid_files = TRAIN_FILENAMES[len_train:]\n",
        "            history_fold, val_fold = train_fold(seed=seed, train_files=train_files, valid_files=valid_files, strategy=STRATEGY)\n",
        "\n",
        "        history[seed] = history_fold\n",
        "        val[seed] = val_fold\n",
        "\n",
        "    return history, val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-LgnQQo8fJi",
        "outputId": "85616c3e-2500-4977-9883-2ffd7a319270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Training with seed: {42}  data:  part\n",
            "# Seeding\n",
            "# Getting Dataset\n",
            "# Done\n",
            "# Getting Dataset\n",
            "# Done\n",
            "# Getting model\n",
            "# Compiling model\n",
            "# Done\n",
            "# Starting training\n",
            "Epoch 1/50\n",
            "    286/Unknown - 1438s 5s/step - loss: 78.6927"
          ]
        }
      ],
      "source": [
        "history, val = run_training()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}